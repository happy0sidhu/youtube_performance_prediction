{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJt39GyWU1BL",
        "outputId": "85c3c6b0-e7a7-4a7a-dfd2-3a115215def2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting YouTube data collection...\n",
            "Total API keys available: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting data: 100%|██████████| 10000/10000 [14:56<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to youtube_data/youtube_video_data_20250625_132841_shuffled.csv with 10000 records\n",
            "\n",
            "Final Data Collection Summary:\n",
            "Total videos collected: 10000\n",
            "Average views: 3,948,005\n",
            "Average likes: 143,416\n",
            "Average days since published: 1.7\n",
            "\n",
            "Records per region:\n",
            "region\n",
            "US    1000\n",
            "IN    1000\n",
            "GB    1000\n",
            "CA    1000\n",
            "AU    1000\n",
            "DE    1000\n",
            "FR    1000\n",
            "BR    1000\n",
            "JP    1000\n",
            "KR    1000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "API keys exhausted: 0/7\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "API_KEYS = [\n",
        "    \"AIzaSyCD2fRqX0HonwyOs_sXKFEKYAceRtNw2N4\",\n",
        "    \"AIzaSyAtRRiS4SlkfAI7lv-1ZXme87B6tTPlMi4\",\n",
        "    \"AIzaSyAX_57FY3W20faltxN11SfoqPWo36eOSAk\",\n",
        "    \"AIzaSyCelaZMPDr6Ag8rkqAyDWjiY96snfc8wKs\",\n",
        "    \"AIzaSyAId-MH2iIq-ngEHKNBtBG7N1tzVbGR0JY\",\n",
        "    \"AIzaSyC_EGQ8DTPPqWCQykXX-4CxCObYVGhmScg\",\n",
        "    \"AIzaSyBL-5jJ3gvegwUatAuSt4i82SOAy9Z4464\"\n",
        "]\n",
        "\n",
        "current_key_index = 0\n",
        "MAX_RETRIES = 3\n",
        "REQUEST_DELAY = 1\n",
        "exhausted_keys = set()\n",
        "\n",
        "def get_next_key():\n",
        "    \"\"\"Rotate through API keys, skipping exhausted ones\"\"\"\n",
        "    global current_key_index\n",
        "    start_index = current_key_index\n",
        "\n",
        "    while True:\n",
        "        key = API_KEYS[current_key_index]\n",
        "        current_key_index = (current_key_index + 1) % len(API_KEYS)\n",
        "\n",
        "        if key not in exhausted_keys:\n",
        "            return key\n",
        "\n",
        "\n",
        "        if current_key_index == start_index:\n",
        "            return None\n",
        "\n",
        "def make_api_request(url, params):\n",
        "    \"\"\"Make API request with retries and key rotation\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        key = get_next_key()\n",
        "        if key is None:\n",
        "            print(\"All API keys exhausted\")\n",
        "            return None\n",
        "\n",
        "        params['key'] = key\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 403:\n",
        "                if 'quotaExceeded' in response.text:\n",
        "                    print(f\"Quota exceeded for key {key}, marking as exhausted\")\n",
        "                    exhausted_keys.add(key)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"Error 403: {response.text}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(f\"Error {response.status_code}: {response.text}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {str(e)}\")\n",
        "            time.sleep(REQUEST_DELAY * (attempt + 1))\n",
        "\n",
        "    print(\"Max retries exceeded for this request\")\n",
        "    return None\n",
        "\n",
        "def get_trending_videos(region_code='US', max_results=50):\n",
        "    \"\"\"Fetch currently trending videos\"\"\"\n",
        "    url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
        "    params = {\n",
        "        'part': 'snippet,contentDetails,statistics,status',\n",
        "        'chart': 'mostPopular',\n",
        "        'regionCode': region_code,\n",
        "        'maxResults': min(max_results, 50),\n",
        "        'hl': 'en'\n",
        "    }\n",
        "\n",
        "    return make_api_request(url, params)\n",
        "\n",
        "def get_video_details(video_ids):\n",
        "    \"\"\"Get details for specific video IDs\"\"\"\n",
        "    url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
        "    params = {\n",
        "        'part': 'snippet,contentDetails,statistics,status',\n",
        "        'id': ','.join(video_ids),\n",
        "        'hl': 'en'\n",
        "    }\n",
        "\n",
        "    return make_api_request(url, params)\n",
        "\n",
        "def get_channel_details(channel_id):\n",
        "    \"\"\"Get channel statistics\"\"\"\n",
        "    url = \"https://www.googleapis.com/youtube/v3/channels\"\n",
        "    params = {\n",
        "        'part': 'statistics',\n",
        "        'id': channel_id\n",
        "    }\n",
        "\n",
        "    return make_api_request(url, params)\n",
        "\n",
        "def process_video_data(video_items, region):\n",
        "    \"\"\"Process raw video data into structured format\"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    for item in video_items:\n",
        "        try:\n",
        "            # Basic video info\n",
        "            video_id = item['id']\n",
        "            snippet = item.get('snippet', {})\n",
        "            stats = item.get('statistics', {})\n",
        "            content_details = item.get('contentDetails', {})\n",
        "            status = item.get('status', {})\n",
        "\n",
        "            channel_id = snippet.get('channelId')\n",
        "            channel_title = snippet.get('channelTitle')\n",
        "\n",
        "            channel_stats = {}\n",
        "            if channel_id:\n",
        "                channel_response = get_channel_details(channel_id)\n",
        "                if channel_response and 'items' in channel_response and channel_response['items']:\n",
        "                    channel_stats = channel_response['items'][0].get('statistics', {})\n",
        "\n",
        "            duration = content_details.get('duration', 'PT0S')\n",
        "            duration_seconds = iso8601_to_seconds(duration)\n",
        "\n",
        "            published_at = snippet.get('publishedAt')\n",
        "            days_since_published = 0\n",
        "            if published_at:\n",
        "                publish_date = datetime.strptime(published_at, '%Y-%m-%dT%H:%M:%SZ')\n",
        "                days_since_published = (datetime.utcnow() - publish_date).days\n",
        "\n",
        "            thumbnails = snippet.get('thumbnails', {})\n",
        "            thumbnail_resolutions = {\n",
        "                'default': thumbnails.get('default', {}).get('url', ''),\n",
        "                'medium': thumbnails.get('medium', {}).get('url', ''),\n",
        "                'high': thumbnails.get('high', {}).get('url', ''),\n",
        "                'standard': thumbnails.get('standard', {}).get('url', ''),\n",
        "                'maxres': thumbnails.get('maxres', {}).get('url', '')\n",
        "            }\n",
        "\n",
        "            video_data = {\n",
        "                'video_id': video_id,\n",
        "                'title': snippet.get('title'),\n",
        "                'description': snippet.get('description'),\n",
        "                'published_at': published_at,\n",
        "                'days_since_published': days_since_published,\n",
        "                'channel_id': channel_id,\n",
        "                'channel_title': channel_title,\n",
        "                'channel_subscribers': int(channel_stats.get('subscriberCount', 0)),\n",
        "                'channel_views': int(channel_stats.get('viewCount', 0)),\n",
        "                'channel_video_count': int(channel_stats.get('videoCount', 0)),\n",
        "                'category_id': snippet.get('categoryId'),\n",
        "                'tags': ','.join(snippet.get('tags', [])),\n",
        "                'duration_seconds': duration_seconds,\n",
        "                'definition': content_details.get('definition'),\n",
        "                'caption': content_details.get('caption'),\n",
        "                'licensed_content': content_details.get('licensedContent', False),\n",
        "                'view_count': int(stats.get('viewCount', 0)),\n",
        "                'like_count': int(stats.get('likeCount', 0)),\n",
        "                'dislike_count': int(stats.get('dislikeCount', 0)),\n",
        "                'comment_count': int(stats.get('commentCount', 0)),\n",
        "                'favorite_count': int(stats.get('favoriteCount', 0)),\n",
        "                'embeddable': status.get('embeddable', False),\n",
        "                'public_stats_viewable': status.get('publicStatsViewable', False),\n",
        "                'made_for_kids': status.get('madeForKids', False),\n",
        "                'region': region,\n",
        "                **thumbnail_resolutions\n",
        "            }\n",
        "\n",
        "            processed_data.append(video_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video {item.get('id')}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def iso8601_to_seconds(duration):\n",
        "    \"\"\"Convert ISO 8601 duration to seconds\"\"\"\n",
        "    try:\n",
        "        duration = duration[2:]\n",
        "        seconds = 0\n",
        "\n",
        "        if 'H' in duration:\n",
        "            hours_part = duration.split('H')[0]\n",
        "            seconds += int(hours_part) * 3600\n",
        "            duration = duration[len(hours_part)+1:]\n",
        "\n",
        "        if 'M' in duration:\n",
        "            minutes_part = duration.split('M')[0]\n",
        "            seconds += int(minutes_part) * 60\n",
        "            duration = duration[len(minutes_part)+1:]\n",
        "\n",
        "        if 'S' in duration:\n",
        "            seconds_part = duration.split('S')[0]\n",
        "            seconds += int(seconds_part)\n",
        "\n",
        "        return seconds\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def collect_data(regions=['US', 'IN', 'GB', 'CA', 'AU', 'DE', 'FR', 'BR', 'JP', 'KR']):\n",
        "    \"\"\"Main function to collect data from multiple regions until quotas are exhausted\"\"\"\n",
        "    all_data = pd.DataFrame()\n",
        "    region_data_counts = {region: 0 for region in regions}\n",
        "\n",
        "    os.makedirs('youtube_data', exist_ok=True)\n",
        "\n",
        "    with tqdm(total=len(regions)*1000, desc=\"Collecting data\") as pbar:\n",
        "        while len(exhausted_keys) < len(API_KEYS):\n",
        "            for region in regions:\n",
        "                if region_data_counts[region] >= 1000:\n",
        "                    continue\n",
        "\n",
        "                remaining = 1000 - region_data_counts[region]\n",
        "                max_results = min(50, remaining)\n",
        "\n",
        "                trending_response = get_trending_videos(region_code=region, max_results=max_results)\n",
        "\n",
        "                if not trending_response:\n",
        "                    print(f\"Failed to get data for region {region}\")\n",
        "                    continue\n",
        "\n",
        "                if 'items' not in trending_response:\n",
        "                    print(f\"No items in response for region {region}\")\n",
        "                    continue\n",
        "\n",
        "                video_items = trending_response['items']\n",
        "                processed_data = process_video_data(video_items, region)  # Pass region here\n",
        "\n",
        "                if processed_data:\n",
        "                    new_data = pd.DataFrame(processed_data)\n",
        "                    region_data_counts[region] += len(new_data)\n",
        "\n",
        "                    all_data = pd.concat([all_data, new_data], ignore_index=True)\n",
        "                    pbar.update(len(new_data))\n",
        "\n",
        "                if all(count >= 1000 for count in region_data_counts.values()):\n",
        "                    break\n",
        "\n",
        "                time.sleep(REQUEST_DELAY)\n",
        "\n",
        "            if all(count >= 1000 for count in region_data_counts.values()):\n",
        "                break\n",
        "\n",
        "    return all_data\n",
        "\n",
        "def save_to_csv(df, filename):\n",
        "    \"\"\"Save collected data to CSV with shuffled records\"\"\"\n",
        "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    shuffled_df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename} with {len(shuffled_df)} records\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting YouTube data collection...\")\n",
        "    print(f\"Total API keys available: {len(API_KEYS)}\")\n",
        "\n",
        "    video_data = collect_data()\n",
        "\n",
        "    if not video_data.empty:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"youtube_data/youtube_video_data_{timestamp}_shuffled.csv\"\n",
        "        save_to_csv(video_data, filename)\n",
        "\n",
        "        print(\"\\nFinal Data Collection Summary:\")\n",
        "        print(f\"Total videos collected: {len(video_data)}\")\n",
        "        print(f\"Average views: {video_data['view_count'].mean():,.0f}\")\n",
        "        print(f\"Average likes: {video_data['like_count'].mean():,.0f}\")\n",
        "        print(f\"Average days since published: {video_data['days_since_published'].mean():.1f}\")\n",
        "        print(\"\\nRecords per region:\")\n",
        "        print(video_data['region'].value_counts())\n",
        "        print(f\"\\nAPI keys exhausted: {len(exhausted_keys)}/{len(API_KEYS)}\")\n",
        "    else:\n",
        "        print(\"No data was collected.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7434T_4VGF7",
        "outputId": "d3ec41ca-f690-42e8-e350-a3cc5ac0510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se0Ol2DxVLWp",
        "outputId": "743c4b42-99de-4b9c-c73e-1b412275a2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNGIldDXVOV_",
        "outputId": "7dd9e5c4-60c8-4bcd-a50e-c6493e015d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dvo_Fq8VUYQ",
        "outputId": "73488ce2-dace-4d40-94e6-19b9a3dc666c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf4PF3BqU6ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2ac803-3610-4566-a28a-bc1f50eeed13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic data cleaning...\n",
            "Extracting NLP features...\n",
            "Processing titles...\n",
            "Processing descriptions...\n",
            "Analyzing tags...\n",
            "Calculating correlations...\n",
            "Creating visualizations...\n",
            "\n",
            "Top Positive Correlations with View Count:\n",
            "view_count                 1.000000\n",
            "like_count                 0.994820\n",
            "channel_subscribers        0.973231\n",
            "comment_count              0.873276\n",
            "channel_views              0.765968\n",
            "title_textblob_polarity    0.510496\n",
            "title_sentiment_pos        0.414290\n",
            "Name: view_count, dtype: float64\n",
            "\n",
            "Top Negative Correlations with View Count:\n",
            "desc_avg_word_length   -0.104113\n",
            "tag_count              -0.109293\n",
            "title_text_length      -0.145141\n",
            "Name: view_count, dtype: float64\n",
            "Saving enhanced dataset...\n",
            "\n",
            "Analysis complete!\n",
            "Enhanced dataset saved with 56 features\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv('/content/youtube_video_data_20250625_125145.csv')\n",
        "\n",
        "print(\"Basic data cleaning...\")\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['description'] = df['description'].str.lower()\n",
        "\n",
        "df['description'] = df['description'].fillna('')\n",
        "df['tags'] = df['tags'].fillna('')\n",
        "\n",
        "print(\"Extracting NLP features...\")\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def extract_text_features(text):\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if w not in stop_words and w not in string.punctuation]\n",
        "\n",
        "    return {\n",
        "        'text_length': len(text),\n",
        "        'word_count': len(words),\n",
        "        'unique_words': len(set(words)),\n",
        "        'avg_word_length': np.mean([len(w) for w in words]) if words else 0,\n",
        "        'sentiment_pos': sentiment['pos'],\n",
        "        'sentiment_neg': sentiment['neg'],\n",
        "        'sentiment_neu': sentiment['neu'],\n",
        "        'sentiment_compound': sentiment['compound'],\n",
        "        'textblob_polarity': blob.sentiment.polarity,\n",
        "        'textblob_subjectivity': blob.sentiment.subjectivity,\n",
        "        'has_question': 1 if '?' in text else 0,\n",
        "        'has_exclamation': 1 if '!' in text else 0,\n",
        "        'has_url': 1 if ('http://' in text or 'https://' in text) else 0\n",
        "    }\n",
        "\n",
        "print(\"Processing titles...\")\n",
        "title_features = pd.DataFrame(df['title'].apply(extract_text_features).tolist())\n",
        "title_features.columns = ['title_' + col for col in title_features.columns]\n",
        "\n",
        "print(\"Processing descriptions...\")\n",
        "desc_features = pd.DataFrame(df['description'].apply(extract_text_features).tolist())\n",
        "desc_features.columns = ['desc_' + col for col in desc_features.columns]\n",
        "\n",
        "df = pd.concat([df, title_features, desc_features], axis=1)\n",
        "\n",
        "print(\"Analyzing tags...\")\n",
        "df['tag_count'] = df['tags'].apply(lambda x: len(x.split(',')) if x else 0)\n",
        "\n",
        "print(\"Calculating correlations...\")\n",
        "\n",
        "numerical_features = [\n",
        "    'view_count', 'like_count', 'dislike_count', 'comment_count',\n",
        "    'days_since_published', 'duration_seconds',\n",
        "    'channel_subscribers', 'channel_views', 'channel_video_count',\n",
        "    'title_text_length', 'title_word_count', 'title_unique_words',\n",
        "    'title_avg_word_length', 'title_sentiment_pos', 'title_sentiment_neg',\n",
        "    'title_sentiment_compound', 'title_textblob_polarity',\n",
        "    'desc_text_length', 'desc_word_count', 'desc_unique_words',\n",
        "    'desc_avg_word_length', 'desc_sentiment_pos', 'desc_sentiment_neg',\n",
        "    'desc_sentiment_compound', 'desc_textblob_polarity',\n",
        "    'tag_count'\n",
        "]\n",
        "\n",
        "numerical_features = [f for f in numerical_features if f in df.columns]\n",
        "\n",
        "corr_matrix = df[numerical_features].corr()\n",
        "\n",
        "print(\"Creating visualizations...\")\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "            center=0, vmin=-1, vmax=1, linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "if 'view_count' in corr_matrix.columns:\n",
        "    view_correlations = corr_matrix['view_count'].sort_values(ascending=False)\n",
        "    print(\"\\nTop Positive Correlations with View Count:\")\n",
        "    print(view_correlations[view_correlations > 0.3].head(10))\n",
        "\n",
        "    print(\"\\nTop Negative Correlations with View Count:\")\n",
        "    print(view_correlations[view_correlations < -0.1].head(10))\n",
        "\n",
        "print(\"Saving enhanced dataset...\")\n",
        "df.to_csv('youtube_data_with_nlp_features.csv', index=False)\n",
        "\n",
        "print(\"\\nAnalysis complete!\")\n",
        "print(f\"Enhanced dataset saved with {len(df.columns)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('youtube_data_with_nlp_features.csv')\n",
        "\n",
        "print(\"\\nExploratory Data Analysis...\")\n",
        "\n",
        "df = df.dropna(subset=['view_count'])\n",
        "df = df[~np.isinf(df['view_count'])]\n",
        "\n",
        "features = [\n",
        "    'like_count', 'dislike_count', 'comment_count',\n",
        "    'days_since_published', 'duration_seconds',\n",
        "    'channel_subscribers', 'channel_views', 'channel_video_count',\n",
        "    'title_text_length', 'title_word_count', 'title_unique_words',\n",
        "    'title_avg_word_length', 'title_sentiment_pos', 'title_sentiment_neg',\n",
        "    'title_sentiment_compound', 'title_textblob_polarity',\n",
        "    'desc_text_length', 'desc_word_count', 'desc_unique_words',\n",
        "    'desc_avg_word_length', 'desc_sentiment_pos', 'desc_sentiment_neg',\n",
        "    'desc_sentiment_compound', 'desc_textblob_polarity',\n",
        "    'tag_count'\n",
        "]\n",
        "\n",
        "features = [f for f in features if f in df.columns]\n",
        "\n",
        "print(\"Missing values in selected features:\")\n",
        "print(df[features].isnull().sum())\n",
        "\n",
        "for col in features:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "print(\"\\nPerforming feature selection...\")\n",
        "X = df[features]\n",
        "y = df['view_count']\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=15)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "X = df[selected_features]\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "preprocessor = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "print(\"\\nTraining models...\")\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in zip(models.keys(), models.values()):\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'pipeline': pipeline,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'predictions': y_pred\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R2 Score: {r2:.2f}\")\n",
        "\n",
        "print(\"\\nPerforming hyperparameter tuning...\")\n",
        "rf_params = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [10, 20, None],\n",
        "    'model__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    rf_pipeline,\n",
        "    rf_params,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "tuned_mse = mean_squared_error(y_test, y_pred_tuned)\n",
        "tuned_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
        "tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(\"\\nTuned Random Forest Results:\")\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"MSE: {tuned_mse:.2f}\")\n",
        "print(f\"MAE: {tuned_mae:.2f}\")\n",
        "print(f\"R2 Score: {tuned_r2:.2f}\")\n",
        "\n",
        "print(\"\\nPerforming cross-validation...\")\n",
        "cv_scores = cross_val_score(\n",
        "    best_model,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring='r2'\n",
        ")\n",
        "print(f\"Cross-validation R2 scores: {cv_scores}\")\n",
        "print(f\"Mean CV R2 score: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
        "\n",
        "print(\"\\nAnalyzing feature importance...\")\n",
        "if isinstance(best_model.named_steps['model'], RandomForestRegressor):\n",
        "    importances = best_model.named_steps['model'].feature_importances_\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': selected_features,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nPerforming residual analysis...\")\n",
        "residuals = y_test - y_pred_tuned\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred_tuned, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.tight_layout()\n",
        "plt.savefig('residual_plot.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nFinal Model Evaluation Summary:\")\n",
        "print(\"Model: Tuned Random Forest\")\n",
        "print(f\"MSE: {tuned_mse:.2f}\")\n",
        "print(f\"MAE: {tuned_mae:.2f}\")\n",
        "print(f\"R2 Score: {tuned_r2:.2f}\")\n",
        "print(f\"Cross-validation R2 Mean: {cv_scores.mean():.2f}\")\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "print(feature_importance.head().to_string())\n",
        "\n",
        "print(\"\\nSaving model...\")\n",
        "joblib.dump(best_model, 'youtube_views_model.pkl')\n",
        "\n",
        "print(\"\\nMachine Learning pipeline complete!\")\n",
        "print(\"Model saved as 'youtube_views_model.pkl'\")\n",
        "print(\"Visualizations saved: correlation_matrix.png, feature_importance.png, residual_plot.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "VHHWt9O-SqPu",
        "outputId": "966d01ef-4546-4fe1-ef35-a977d14489c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'youtube_data_with_nlp_features.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2838183668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'youtube_data_with_nlp_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExploratory Data Analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'youtube_data_with_nlp_features.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('youtube_data_with_nlp_features.csv')\n",
        "\n",
        "print(\"\\nExploratory Data Analysis...\")\n",
        "df = df.dropna(subset=['view_count'])\n",
        "df = df[~np.isinf(df['view_count'])]\n",
        "\n",
        "y = np.log1p(df['view_count'])\n",
        "\n",
        "features = [\n",
        "    'days_since_published', 'duration_seconds',\n",
        "    'channel_subscribers', 'channel_views', 'channel_video_count',\n",
        "    'title_text_length', 'title_word_count', 'title_unique_words',\n",
        "    'title_avg_word_length', 'title_sentiment_pos', 'title_sentiment_neg',\n",
        "    'title_sentiment_compound', 'title_textblob_polarity',\n",
        "    'desc_text_length', 'desc_word_count', 'desc_unique_words',\n",
        "    'desc_avg_word_length', 'desc_sentiment_pos', 'desc_sentiment_neg',\n",
        "    'desc_sentiment_compound', 'desc_textblob_polarity',\n",
        "    'tag_count'\n",
        "]\n",
        "\n",
        "features = [f for f in features if f in df.columns]\n",
        "\n",
        "print(\"Missing values in selected features:\")\n",
        "print(df[features].isnull().sum())\n",
        "\n",
        "for col in features:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "print(\"\\nPerforming feature selection...\")\n",
        "X = df[features]\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=15)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "X = df[selected_features]\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "preprocessor = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "print(\"\\nTraining models...\")\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in zip(models.keys(), models.values()):\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    y_test_inv = np.expm1(y_test)\n",
        "    y_pred_inv = np.expm1(y_pred)\n",
        "\n",
        "    mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    percentage_error = np.mean(np.abs((y_test_inv - y_pred_inv) / y_test_inv)) * 100\n",
        "\n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'pipeline': pipeline,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'percentage_error': percentage_error,\n",
        "        'predictions': y_pred\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"MSE: {mse:,.2f}\")\n",
        "    print(f\"MAE: {mae:,.2f}\")\n",
        "    print(f\"RMSE: {rmse:,.2f}\")\n",
        "    print(f\"R2 Score: {r2:.2f}\")\n",
        "    print(f\"Percentage Error: {percentage_error:.2f}%\")\n",
        "\n",
        "print(\"\\nPerforming hyperparameter tuning...\")\n",
        "rf_params = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [10, 20, 30, None],\n",
        "    'model__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    rf_pipeline,\n",
        "    rf_params,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "y_test_inv_tuned = np.expm1(y_test)\n",
        "y_pred_inv_tuned = np.expm1(y_pred_tuned)\n",
        "\n",
        "tuned_mse = mean_squared_error(y_test_inv_tuned, y_pred_inv_tuned)\n",
        "tuned_mae = mean_absolute_error(y_test_inv_tuned, y_pred_inv_tuned)\n",
        "tuned_rmse = np.sqrt(tuned_mse)\n",
        "tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
        "tuned_percentage_error = np.mean(np.abs((y_test_inv_tuned - y_pred_inv_tuned) / y_test_inv_tuned)) * 100\n",
        "\n",
        "print(\"\\nTuned Random Forest Results:\")\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"MSE: {tuned_mse:,.2f}\")\n",
        "print(f\"MAE: {tuned_mae:,.2f}\")\n",
        "print(f\"RMSE: {tuned_rmse:,.2f}\")\n",
        "print(f\"R2 Score: {tuned_r2:.2f}\")\n",
        "print(f\"Percentage Error: {tuned_percentage_error:.2f}%\")\n",
        "\n",
        "print(\"\\nPerforming cross-validation...\")\n",
        "cv_scores = cross_val_score(\n",
        "    best_model,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring='r2'\n",
        ")\n",
        "print(f\"Cross-validation R2 scores: {cv_scores}\")\n",
        "print(f\"Mean CV R2 score: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
        "\n",
        "print(\"\\nAnalyzing feature importance...\")\n",
        "if isinstance(best_model.named_steps['model'], RandomForestRegressor):\n",
        "    importances = best_model.named_steps['model'].feature_importances_\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': selected_features,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nPerforming residual analysis...\")\n",
        "residuals = y_test - y_pred_tuned\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred_tuned, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Log(View Count)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.tight_layout()\n",
        "plt.savefig('residual_plot.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nFinal Model Evaluation Summary:\")\n",
        "print(\"Model: Tuned Random Forest\")\n",
        "print(f\"MSE: {tuned_mse:,.2f}\")\n",
        "print(f\"MAE: {tuned_mae:,.2f}\")\n",
        "print(f\"RMSE: {tuned_rmse:,.2f}\")\n",
        "print(f\"R2 Score: {tuned_r2:.2f}\")\n",
        "print(f\"Percentage Error: {tuned_percentage_error:.2f}%\")\n",
        "print(f\"Cross-validation R2 Mean: {cv_scores.mean():.2f}\")\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "print(feature_importance.head().to_string())\n",
        "\n",
        "print(\"\\nSaving model...\")\n",
        "joblib.dump(best_model, 'youtube_views_model_improved.pkl')\n",
        "\n",
        "print(\"\\nMachine Learning pipeline complete!\")\n",
        "print(\"Model saved as 'youtube_views_model_improved.pkl'\")\n",
        "print(\"Visualizations saved: feature_importance.png, residual_plot.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5plUBqUWbgx",
        "outputId": "58973f33-802e-402e-ae71-386591947420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "\n",
            "Exploratory Data Analysis...\n",
            "Missing values in selected features:\n",
            "days_since_published        0\n",
            "duration_seconds            0\n",
            "channel_subscribers         0\n",
            "channel_views               0\n",
            "channel_video_count         0\n",
            "title_text_length           0\n",
            "title_word_count            0\n",
            "title_unique_words          0\n",
            "title_avg_word_length       0\n",
            "title_sentiment_pos         0\n",
            "title_sentiment_neg         0\n",
            "title_sentiment_compound    0\n",
            "title_textblob_polarity     0\n",
            "desc_text_length            0\n",
            "desc_word_count             0\n",
            "desc_unique_words           0\n",
            "desc_avg_word_length        0\n",
            "desc_sentiment_pos          0\n",
            "desc_sentiment_neg          0\n",
            "desc_sentiment_compound     0\n",
            "desc_textblob_polarity      0\n",
            "tag_count                   0\n",
            "dtype: int64\n",
            "\n",
            "Performing feature selection...\n",
            "Selected features: ['days_since_published', 'channel_subscribers', 'channel_views', 'title_avg_word_length', 'title_sentiment_pos', 'title_sentiment_neg', 'title_sentiment_compound', 'title_textblob_polarity', 'desc_text_length', 'desc_word_count', 'desc_unique_words', 'desc_sentiment_pos', 'desc_sentiment_neg', 'desc_sentiment_compound', 'tag_count']\n",
            "\n",
            "Splitting data...\n",
            "\n",
            "Training models...\n",
            "\n",
            "Linear Regression Results:\n",
            "MSE: 47,989,823,069,366.20\n",
            "MAE: 1,587,220.58\n",
            "RMSE: 6,927,468.73\n",
            "R2 Score: 0.39\n",
            "Percentage Error: 122.92%\n",
            "\n",
            "Random Forest Results:\n",
            "MSE: 1,411,667,963,523.47\n",
            "MAE: 431,747.80\n",
            "RMSE: 1,188,136.34\n",
            "R2 Score: 0.66\n",
            "Percentage Error: 85.18%\n",
            "\n",
            "XGBoost Results:\n",
            "MSE: 989,714,035,285.41\n",
            "MAE: 403,670.78\n",
            "RMSE: 994,843.72\n",
            "R2 Score: 0.65\n",
            "Percentage Error: 81.85%\n",
            "\n",
            "Performing hyperparameter tuning...\n",
            "\n",
            "Tuned Random Forest Results:\n",
            "Best parameters: {'model__max_depth': 20, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
            "MSE: 1,421,967,270,281.21\n",
            "MAE: 431,962.96\n",
            "RMSE: 1,192,462.69\n",
            "R2 Score: 0.66\n",
            "Percentage Error: 85.67%\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-validation R2 scores: [0.6354943  0.95715646 0.87491335 0.56449813 0.46920144]\n",
            "Mean CV R2 score: 0.70 (+/- 0.37)\n",
            "\n",
            "Analyzing feature importance...\n",
            "\n",
            "Performing residual analysis...\n",
            "\n",
            "Final Model Evaluation Summary:\n",
            "Model: Tuned Random Forest\n",
            "MSE: 1,421,967,270,281.21\n",
            "MAE: 431,962.96\n",
            "RMSE: 1,192,462.69\n",
            "R2 Score: 0.66\n",
            "Percentage Error: 85.67%\n",
            "Cross-validation R2 Mean: 0.70\n",
            "\n",
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "1    channel_subscribers    0.453807\n",
            "2          channel_views    0.125162\n",
            "0   days_since_published    0.066238\n",
            "14             tag_count    0.063087\n",
            "8       desc_text_length    0.058508\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Machine Learning pipeline complete!\n",
            "Model saved as 'youtube_views_model_improved.pkl'\n",
            "Visualizations saved: feature_importance.png, residual_plot.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('youtube_data_with_nlp_features.csv')\n",
        "\n",
        "print(\"\\nExploratory Data Analysis...\")\n",
        "df = df.dropna(subset=['view_count'])\n",
        "df = df[~np.isinf(df['view_count'])]\n",
        "\n",
        "view_count_99 = np.percentile(df['view_count'], 99)\n",
        "df['view_count_capped'] = np.where(df['view_count'] > view_count_99, view_count_99, df['view_count'])\n",
        "y = np.log1p(df['view_count_capped'])\n",
        "\n",
        "base_features = [\n",
        "    'days_since_published', 'duration_seconds',\n",
        "    'channel_subscribers', 'channel_views', 'channel_video_count',\n",
        "    'title_text_length', 'title_word_count', 'title_unique_words',\n",
        "    'title_avg_word_length', 'title_sentiment_pos', 'title_sentiment_neg',\n",
        "    'title_sentiment_compound', 'title_textblob_polarity',\n",
        "    'desc_text_length', 'desc_word_count', 'desc_unique_words',\n",
        "    'desc_avg_word_length', 'desc_sentiment_pos', 'desc_sentiment_neg',\n",
        "    'desc_sentiment_compound', 'desc_textblob_polarity',\n",
        "    'tag_count'\n",
        "]\n",
        "categorical_features = ['category_id', 'region']\n",
        "text_features = ['title', 'description']\n",
        "\n",
        "base_features = [f for f in base_features if f in df.columns]\n",
        "categorical_features = [f for f in categorical_features if f in df.columns]\n",
        "\n",
        "for col in base_features:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "print(\"\\nEnhancing feature engineering...\")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "title_tfidf = tfidf.fit_transform(df['title'].fillna('')).toarray()\n",
        "desc_tfidf = tfidf.fit_transform(df['description'].fillna('')).toarray()\n",
        "tfidf_df = pd.DataFrame(np.hstack((title_tfidf, desc_tfidf)), columns=[f'tfidf_{i}' for i in range(200)])\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "cat_encoded = encoder.fit_transform(df[categorical_features])\n",
        "cat_df = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "X = pd.concat([df[base_features], tfidf_df, cat_df], axis=1)\n",
        "\n",
        "print(\"\\nPerforming feature selection...\")\n",
        "selector = SelectKBest(score_func=f_regression, k=20)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "print(\"Selected features:\", selected_features)\n",
        "X = X[selected_features]\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "preprocessor = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "print(\"\\nTraining models for ensemble...\")\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "pipelines = {}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    pipelines[name] = pipeline\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    y_test_inv = np.expm1(y_test)\n",
        "    y_pred_inv = np.expm1(y_pred)\n",
        "\n",
        "    mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    weights = 1 / y_test_inv\n",
        "    weighted_mae = np.average(np.abs(y_test_inv - y_pred_inv), weights=weights)\n",
        "\n",
        "    results[name] = {\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'weighted_mae': weighted_mae\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"MSE: {mse:,.2f}\")\n",
        "    print(f\"MAE: {mae:,.2f}\")\n",
        "    print(f\"RMSE: {rmse:,.2f}\")\n",
        "    print(f\"R2 Score: {r2:.2f}\")\n",
        "    print(f\"Weighted MAE: {weighted_mae:,.2f}\")\n",
        "\n",
        "ensemble_pred = (pipelines['Random Forest'].predict(X_test) + pipelines['XGBoost'].predict(X_test)) / 2\n",
        "y_test_inv = np.expm1(y_test)\n",
        "y_pred_ensemble_inv = np.expm1(ensemble_pred)\n",
        "\n",
        "ensemble_mse = mean_squared_error(y_test_inv, y_pred_ensemble_inv)\n",
        "ensemble_mae = mean_absolute_error(y_test_inv, y_pred_ensemble_inv)\n",
        "ensemble_rmse = np.sqrt(ensemble_mse)\n",
        "ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
        "ensemble_weights = 1 / y_test_inv\n",
        "ensemble_weighted_mae = np.average(np.abs(y_test_inv - y_pred_ensemble_inv), weights=ensemble_weights)\n",
        "\n",
        "print(f\"\\nEnsemble Results:\")\n",
        "print(f\"MSE: {ensemble_mse:,.2f}\")\n",
        "print(f\"MAE: {ensemble_mae:,.2f}\")\n",
        "print(f\"RMSE: {ensemble_rmse:,.2f}\")\n",
        "print(f\"R2 Score: {ensemble_r2:.2f}\")\n",
        "print(f\"Weighted MAE: {ensemble_weighted_mae:,.2f}\")\n",
        "\n",
        "print(\"\\nPerforming stabilized cross-validation...\")\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(pipelines['Random Forest'], X, y, cv=kf, scoring='r2')\n",
        "print(f\"Cross-validation R2 scores: {cv_scores}\")\n",
        "print(f\"Mean CV R2 score: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
        "\n",
        "print(\"\\nAnalyzing feature importance...\")\n",
        "importances = pipelines['Random Forest'].named_steps['model'].feature_importances_\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
        "plt.title('Top 10 Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance222.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nPerforming residual analysis...\")\n",
        "residuals = y_test - ensemble_pred\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(ensemble_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Log(View Count)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.tight_layout()\n",
        "plt.savefig('residual_plot222.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nFinal Model Evaluation Summary:\")\n",
        "print(\"Model: Ensemble (Random Forest + XGBoost)\")\n",
        "print(f\"MSE: {ensemble_mse:,.2f}\")\n",
        "print(f\"MAE: {ensemble_mae:,.2f}\")\n",
        "print(f\"RMSE: {ensemble_rmse:,.2f}\")\n",
        "print(f\"R2 Score: {ensemble_r2:.2f}\")\n",
        "print(f\"Weighted MAE: {ensemble_weighted_mae:,.2f}\")\n",
        "print(f\"Cross-validation R2 Mean: {cv_scores.mean():.2f}\")\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "print(feature_importance.head().to_string())\n",
        "\n",
        "print(\"\\nSaving model...\")\n",
        "joblib.dump(pipelines, 'youtube_views_model_enhanced222.pkl')\n",
        "\n",
        "print(\"\\nMachine Learning pipeline complete!\")\n",
        "print(\"Model saved as 'youtube_views_model_enhanced222.pkl'\")\n",
        "print(\"Visualizations saved: feature_importance.png, residual_plot.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHEjcZy0YJr-",
        "outputId": "755b2aa2-7a3f-4b45-ae24-6a6048b28fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "\n",
            "Exploratory Data Analysis...\n",
            "\n",
            "Enhancing feature engineering...\n",
            "\n",
            "Performing feature selection...\n",
            "Selected features: ['days_since_published', 'channel_subscribers', 'channel_views', 'title_sentiment_pos', 'title_textblob_polarity', 'tfidf_0', 'tfidf_2', 'tfidf_4', 'tfidf_49', 'tfidf_53', 'tfidf_98', 'tfidf_115', 'tfidf_149', 'tfidf_150', 'tfidf_161', 'tfidf_163', 'tfidf_169', 'tfidf_175', 'tfidf_181', 'tfidf_193']\n",
            "\n",
            "Splitting data...\n",
            "\n",
            "Training models for ensemble...\n",
            "\n",
            "Random Forest Results:\n",
            "MSE: 1,281,868,547,382.60\n",
            "MAE: 402,124.43\n",
            "RMSE: 1,132,196.34\n",
            "R2 Score: 0.73\n",
            "Weighted MAE: 204,898.46\n",
            "\n",
            "XGBoost Results:\n",
            "MSE: 1,153,882,086,663.11\n",
            "MAE: 404,258.58\n",
            "RMSE: 1,074,189.04\n",
            "R2 Score: 0.68\n",
            "Weighted MAE: 208,552.27\n",
            "\n",
            "Ensemble Results:\n",
            "MSE: 1,211,096,474,917.07\n",
            "MAE: 402,031.90\n",
            "RMSE: 1,100,498.28\n",
            "R2 Score: 0.72\n",
            "Weighted MAE: 200,580.86\n",
            "\n",
            "Performing stabilized cross-validation...\n",
            "Cross-validation R2 scores: [0.65830453 0.77903349 0.80534478 0.78861235 0.61793222 0.7153122\n",
            " 0.75948216 0.90791869 0.46922528 0.87482972]\n",
            "Mean CV R2 score: 0.74 (+/- 0.25)\n",
            "\n",
            "Analyzing feature importance...\n",
            "\n",
            "Performing residual analysis...\n",
            "\n",
            "Final Model Evaluation Summary:\n",
            "Model: Ensemble (Random Forest + XGBoost)\n",
            "MSE: 1,211,096,474,917.07\n",
            "MAE: 402,031.90\n",
            "RMSE: 1,100,498.28\n",
            "R2 Score: 0.72\n",
            "Weighted MAE: 200,580.86\n",
            "Cross-validation R2 Mean: 0.74\n",
            "\n",
            "Top 5 Important Features:\n",
            "                    Feature  Importance\n",
            "1       channel_subscribers    0.426195\n",
            "2             channel_views    0.181710\n",
            "0      days_since_published    0.080945\n",
            "16                tfidf_169    0.076808\n",
            "4   title_textblob_polarity    0.034095\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Machine Learning pipeline complete!\n",
            "Model saved as 'youtube_views_model_enhanced222.pkl'\n",
            "Visualizations saved: feature_importance.png, residual_plot.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iw7rvBFiZp-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}